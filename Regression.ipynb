{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is Simple Linear Regression?**\n",
        "\n",
        "  Simple Linear Regression models the relationship between two variables: one independent variable (X) and one dependent variable (Y), using a straight line:\n",
        "  Y = mX + c\n",
        "\n",
        "**2.What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "  Linearity: The relationship between X and Y is linear.\n",
        "  Independence: Observations are independent.\n",
        "  Homoscedasticity: Constant variance of errors.\n",
        "  Normality: Errors are normally distributed.\n",
        "  No multicollinearity (trivial in simple regression).\n",
        "\n",
        "**3.What does the coefficient m represent in the equation Y=mX+c?**\n",
        "\n",
        "  m is the slope, indicating the change in Y for a one-unit increase in X.\n",
        "\n",
        "**4.What does the intercept c represent in the equation Y=mX+c?**\n",
        "\n",
        "  c is the intercept, representing the predicted value of Y when X = 0.\n",
        "\n",
        "**5.How do we calculate the slope m in Simple Linear Regression?**\n",
        "\n",
        "  m= n∑XY-∑X∑Y/(n∑X2-(∑X)2)\n",
        "\n",
        "**6.What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "  To minimize the sum of squared residuals (difference between actual and predicted Y).\n",
        "\n",
        "**7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "\n",
        "  R² indicates the proportion of variance in Y explained by X. Values range from 0 to 1.\n",
        "\n",
        "**8.What is Multiple Linear Regression?**\n",
        "\n",
        "  It models the relationship between one dependent variable and two or more independent variables:\n",
        "  Y = b₀ + b₁X₁ + b₂X₂ + ... + bₙXₙ\n",
        "\n",
        "**9.What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "  Simple: One independent variable.\n",
        "  Multiple: Two or more independent variables\n",
        "\n",
        "**10.What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "  Linearity\n",
        "  Independence of errors\n",
        "  Homoscedasticity\n",
        "  Normality of errors\n",
        "  No multicollinearity\n",
        "\n",
        "**11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "\n",
        "  It's when residuals have non-constant variance, which can bias standard errors and significance tests.\n",
        "\n",
        "**12.How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "  Remove highly correlated variables\n",
        "  Use Principal Component Analysis (PCA)\n",
        "  Apply Ridge or Lasso regression\n",
        "\n",
        "**13.What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "\n",
        "  One-Hot Encoding\n",
        "  Label Encoding\n",
        "  Ordinal Encoding (if order matters)\n",
        "\n",
        "**14.What is the role of interaction terms in Multiple Linear Regression?**\n",
        "\n",
        "  They allow the effect of one variable to depend on the value of another, capturing variable interaction:\n",
        "  Y = b₀ + b₁X₁ + b₂X₂ + b₃(X₁*X₂)\n",
        "\n",
        "**15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "\n",
        "  Simple: Y when X = 0.\n",
        "  Multiple: Y when all Xs = 0, which might be unrealistic\n",
        "\n",
        "**16.What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "\n",
        "  Shows how much Y changes per unit increase in X. A significant slope indicates a meaningful relationship.\n",
        "\n",
        "**17.How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "  Provides a baseline value of Y when all Xs are zero—important for model interpretation.\n",
        "\n",
        "**18.What are the limitations of using R² as a sole measure of model performance?**\n",
        "\n",
        "  Doesn't indicate causality\n",
        "  Can be high for poor models\n",
        "  Increases with more predictors, even if they're irrelevant\n",
        "\n",
        "**19.How would you interpret a large standard error for a regression coefficient?**\n",
        "\n",
        "  Implies uncertainty or instability in that coefficient's estimate—could be due to multicollinearity or noise.\n",
        "\n",
        "**20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "  Look for funnel shapes (residuals spreading out). It's important because it violates regression assumptions.\n",
        "\n",
        "**21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "\n",
        "  Means additional variables don't improve the model meaningfully—may be overfitting.\n",
        "\n",
        "**22.Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "  Improves interpretability\n",
        "  Essential for regularized models (e.g., Ridge/Lasso)\n",
        "  Helps with gradient descent convergence\n",
        "\n",
        "**23.What is polynomial regression?**\n",
        "\n",
        "  A regression model where the relationship is modeled as an nth-degree polynomial:\n",
        "  Y = b₀ + b₁X + b₂X² + ... + bₙXⁿ\n",
        "\n",
        "**24.How does polynomial regression differ from linear regression?**\n",
        "\n",
        "  It models nonlinear relationships by including powers of the predictor.\n",
        "\n",
        "**25.When is polynomial regression used?**\n",
        "\n",
        "  When data shows a curved (non-linear) trend that a straight line can't capture.\n",
        "\n",
        "**26.What is the general equation for polynomial regression?**\n",
        "  \n",
        "  Y = b₀ + b₁X + b₂X² + ... + bₙXⁿ\n",
        "\n",
        "**27.Can polynomial regression be applied to multiple variables?**\n",
        "\n",
        "  Yes, it's called multivariate polynomial regression (e.g., includes X₁², X₁X₂ etc)\n",
        "\n",
        "**28.What are the limitations of polynomial regression?**\n",
        "\n",
        "  Overfitting for high degrees\n",
        "  Less interpretable\n",
        "  Sensitive to outliers\n",
        "  Poor extrapolation\n",
        "\n",
        "**29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "  Cross-validation\n",
        "  Adjusted R²\n",
        "  AIC/BIC\n",
        "  Residual analysis\n",
        "\n",
        "**30.Why is visualization important in polynomial regression?**\n",
        "\n",
        "  It helps you see how well the model fits the data, identify overfitting, and understand the trend.\n",
        "\n",
        "**31.How is polynomial regression implemented in Python?**\n",
        "\n",
        "  from sklearn.preprocessing import PolynomialFeatures\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  from sklearn.pipeline import make_pipeline\n",
        "  model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "uabLxSNfoYB-"
      }
    }
  ]
}